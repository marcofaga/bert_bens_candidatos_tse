{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1237,"status":"ok","timestamp":1741331250229,"user":{"displayName":"Marco Antonio Faganello","userId":"02415400589316055254"},"user_tz":180},"id":"7hgvylYAgBMY","outputId":"ad5d680e-6f66-48e8-c43a-e75fccf0be92"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/MAFData/projetosAtivos/2303_posdoc_fgv/jobs/a30 - ML e Bens de Candidatos\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/MAFData/projetosAtivos/2303_posdoc_fgv/jobs/a30 - ML e Bens de Candidatos"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iMBZAu6g-iv8"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import torch\n","from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n","from torch.amp import autocast, GradScaler\n","import torch.optim as optim\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup #hugging face\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n","from tqdm import tqdm\n","from datetime import datetime\n","import random"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wLlyHL0kgOIE"},"outputs":[],"source":["# 1. Definir o Dataset customizado para treino/validação\n","class TextDataset(Dataset):\n","    def __init__(self, dataframe, tokenizer, max_len=128, is_train=True):\n","        self.dataframe = dataframe.reset_index(drop=True)\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","        self.is_train = is_train\n","\n","    def __len__(self):\n","        return len(self.dataframe)\n","\n","    def __getitem__(self, index):\n","        text = self.dataframe.loc[index, 'ds_bem_candidato_2']\n","        encoding = self.tokenizer.encode_plus(\n","            text,\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            truncation=True,\n","            padding='max_length',\n","            return_attention_mask=True,\n","            return_tensors='pt'\n","        )\n","        item = {\n","            'input_ids': encoding['input_ids'].squeeze(),  # remove dimensao extra\n","            'attention_mask': encoding['attention_mask'].squeeze()\n","        }\n","        if self.is_train:\n","            label = self.dataframe.loc[index, 'y']\n","            item['labels'] = torch.tensor(label, dtype=torch.long)\n","        return item\n","\n","# Dataset para aplicação (sem labels)\n","class AppDataset(Dataset):\n","    def __init__(self, dataframe, tokenizer, max_len=128):\n","        self.dataframe = dataframe.reset_index(drop=True)\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.dataframe)\n","\n","    def __getitem__(self, index):\n","        text = self.dataframe.loc[index, 'ds_bem_candidato_2']\n","        encoding = self.tokenizer.encode_plus(\n","            text,\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            truncation=True,\n","            padding='max_length',\n","            return_attention_mask=True,\n","            return_tensors='pt'\n","        )\n","        return {\n","            'input_ids': encoding['input_ids'].squeeze(),\n","            'attention_mask': encoding['attention_mask'].squeeze()\n","        }"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xZdRk1Zuga0Z"},"outputs":[],"source":["# 2. Carregar os dados e dividir em treino e validação\n","file_parquet = 'bases/bd01_benscand_treino.parquet'\n","df = pd.read_parquet(file_parquet)\n","df = df.sample(n=10000, random_state=42)\n","df['y'] = df['y'].astype(int)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sOe68n-9tDel"},"outputs":[],"source":["# Split data\n","\n","# Primeiro, divide em 75% treino e 25% em um conjunto temporário\n","train_df, temp_df = train_test_split(\n","    df, test_size=0.25, random_state=42, stratify=df['y']\n",")\n","\n","# Em seguida, divide o conjunto temporário em 5% validação e 20% teste.\n","val_df, test_df = train_test_split(\n","    temp_df, test_size=0.8, random_state=42, stratify=temp_df['y']\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4769,"status":"ok","timestamp":1741331267434,"user":{"displayName":"Marco Antonio Faganello","userId":"02415400589316055254"},"user_tz":180},"id":"x1kPjpXAgcw7","outputId":"d3946fc7-a1af-40d4-f709-532a3fc5200c"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at neuralmind/bert-large-portuguese-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["# 3. Carregar o tokenizer e o modelo BERT para português\n","model_name = \"neuralmind/bert-large-portuguese-cased\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5)\n","#model = AutoModelForSequenceClassification.from_pretrained(\"modelo/colab/modelo_bert_treinado/\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":490,"status":"ok","timestamp":1741331267972,"user":{"displayName":"Marco Antonio Faganello","userId":"02415400589316055254"},"user_tz":180},"id":"HiLRAKG-mY9D","outputId":"77b96486-9312-4da1-874b-31e5670edf2d"},"outputs":[{"data":{"text/plain":["device(type='cuda')"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["# 4. Configurar o dispositivo (GPU com CUDA, se disponível)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","device"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iIrq6-EjLg9W"},"outputs":[],"source":["# 5. Criar os datasets\n","max_len = 128 # número de tokens que ele vai transformar cada texto\n","batch_size = 50\n","\n","# Criação dos datasets a partir dos dataframes\n","train_dataset = TextDataset(train_df, tokenizer, max_len=max_len, is_train=True)\n","val_dataset   = TextDataset(val_df, tokenizer, max_len=max_len, is_train=True)\n","test_dataset  = TextDataset(test_df, tokenizer, max_len=max_len, is_train=True)\n","\n","# DataLoader para treino, val e test usando o sampler\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers = 0)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers = 0)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers = 0)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cpXmVzHhw2g4"},"outputs":[],"source":["# 6. Configurar o otimizador e scheduler\n","epochs = 50\n","optimizer = optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n","total_steps = len(train_loader) * epochs\n","scheduler = get_linear_schedule_with_warmup(optimizer,\n","                                            num_warmup_steps=int(0.1 * total_steps),\n","                                            num_training_steps=total_steps)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l6Nurr1aziX2","outputId":"2c50ae56-d054-445b-eb0f-30352e521d9b"},"outputs":[{"name":"stdout","output_type":"stream","text":["2025-03-08 01:04:59.785957\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1/50:   7%|████                                                    | 11/150 [00:57<13:05,  5.65s/it, loss=1.6677]"]}],"source":["print(datetime.now())\n","\n","patience = 3\n","best_val_loss = float('inf')\n","patience_counter = 0\n","\n","scaler = GradScaler(\"cuda\")\n","\n","# 7. Loop de treinamento\n","for epoch in range(epochs):\n","\n","    # Inicia o treinamento\n","    model.train()\n","    train_loss = 0.0\n","    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n","\n","    for batch in progress_bar:\n","        optimizer.zero_grad()\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)\n","\n","         # Executa a passagem forward em FP16 onde possível\n","        with autocast(\"cuda\"):\n","          outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","          loss = outputs.loss\n","\n","        # Escala o loss, faz o backward e atualiza os parâmetros\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","        scheduler.step()\n","\n","        train_loss += loss.item()\n","        progress_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n","\n","    avg_train_loss = train_loss / len(train_loader)\n","    print(f\"\\nEpoch {epoch+1} - Treino Loss: {avg_train_loss:.4f}\")\n","\n","    # Avaliação no conjunto de validação\n","    model.eval()\n","    val_loss = 0.0\n","    preds = []\n","    true_labels = []\n","    with torch.no_grad():\n","        for batch in val_loader:\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            labels = batch['labels'].to(device)\n","\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            val_loss += loss.item()\n","            logits = outputs.logits\n","            preds.extend(torch.argmax(logits, dim=1).cpu().numpy())\n","            true_labels.extend(labels.cpu().numpy())\n","\n","    avg_val_loss = val_loss / len(val_loader)\n","    acc = accuracy_score(true_labels, preds)\n","    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, preds, average='weighted')\n","    print(f\"Epoch {epoch+1} - Validação Loss: {avg_val_loss:.4f} | Acurácia: {acc:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f} | F1: {f1:.4f}\\n\")\n","\n","    # Early Stopping: verificar se a loss de validação melhorou\n","    if avg_val_loss < best_val_loss:\n","        best_val_loss = avg_val_loss\n","        patience_counter = 0\n","        model.save_pretrained(\"modelo/colab/modelo_bert_treinado\")\n","        print(f\"Melhora na loss: {best_val_loss}\")\n","    else:\n","        patience_counter += 1\n","        print(f\"Sem melhora na loss de validação por {patience_counter} epoch(s).\")\n","\n","    if patience_counter >= patience:\n","        print(f\"Early stopping: Não houve melhora na loss de validação por {patience} epochs consecutivas.\")\n","        break\n","\n","print(datetime.now())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jC7R1f4SuGwT"},"outputs":[],"source":[]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.13.2"}},"nbformat":4,"nbformat_minor":0}